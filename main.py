import html
import os
import re
import time
from datetime import datetime
from email.utils import parsedate_to_datetime
from typing import Dict, List, Optional

import google.generativeai as genai
import pandas as pd
import requests
from bs4 import BeautifulSoup

EXCLUDE_PUBLISHERS: List[str] = []
EXCLUDE_REPORTERS: List[str] = []

NAVER_API_URL = "https://openapi.naver.com/v1/search/news.json"
QUERY = "ë¶€ë™ì‚° ì „ë§"
TARGET_COUNT = 30
CSV_PATH = "news_data.csv"


def get_env(name: str) -> str:
    value = os.getenv(name)
    if not value:
        raise EnvironmentError(f"í™˜ê²½ë³€ìˆ˜ {name} ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return value


def clean_html(raw_text: str) -> str:
    no_tag = re.sub(r"<[^>]+>", "", raw_text or "")
    return html.unescape(no_tag).strip()


def parse_pub_date(pub_date: str) -> str:
    try:
        dt = parsedate_to_datetime(pub_date)
        return dt.isoformat()
    except Exception:
        return datetime.utcnow().isoformat()


def extract_article_metadata(link: str) -> Dict[str, str]:
    metadata = {"publisher": "Unknown", "reporter": "Unknown", "content": ""}

    try:
        resp = requests.get(link, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        publisher_candidates = [
            soup.select_one("meta[property='og:article:author']"),
            soup.select_one("meta[name='twitter:creator']"),
            soup.select_one("meta[property='og:site_name']"),
            soup.select_one("meta[name='newsct']"),
            soup.select_one("a.media_end_head_top_logo img"),
            soup.select_one("img.media_end_head_top_logo_img"),
            soup.select_one(".press_logo img"),
        ]

        for candidate in publisher_candidates:
            if not candidate:
                continue
            value = candidate.get("content") or candidate.get("alt") or candidate.get_text(strip=True)
            if value:
                metadata["publisher"] = value.strip()
                break

        reporter_candidates = [
            soup.select_one("meta[name='byl']"),
            soup.select_one("meta[property='article:author']"),
            soup.select_one(".media_end_head_journalist_name"),
            soup.select_one(".byline_s"),
            soup.select_one(".article_writer"),
            soup.select_one(".reporter"),
        ]

        for candidate in reporter_candidates:
            if not candidate:
                continue
            value = candidate.get("content") or candidate.get_text(" ", strip=True)
            if value:
                value = re.sub(r"ê¸°ì.*$", "ê¸°ì", value).strip()
                metadata["reporter"] = value
                break

        content_node = (
            soup.select_one("article#dic_area")
            or soup.select_one("#newsct_article")
            or soup.select_one("#articleBodyContents")
            or soup.select_one("article")
            or soup.body
        )
        if content_node:
            text = content_node.get_text(" ", strip=True)
            metadata["content"] = re.sub(r"\s+", " ", text)

    except Exception:
        pass

    return metadata


def fetch_naver_news(client_id: str, client_secret: str) -> List[Dict[str, str]]:
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }

    collected: List[Dict[str, str]] = []
    seen_links = set()

    for start in range(1, 1000, 100):
        params = {
            "query": QUERY,
            "display": 100,
            "start": start,
            "sort": "date",
        }
        res = requests.get(NAVER_API_URL, headers=headers, params=params, timeout=10)
        res.raise_for_status()
        items = res.json().get("items", [])

        if not items:
            break

        for item in items:
            link = item.get("originallink") or item.get("link")
            if not link or link in seen_links:
                continue

            meta = extract_article_metadata(link)
            publisher = meta.get("publisher", "Unknown")
            reporter = meta.get("reporter", "Unknown")

            if publisher in EXCLUDE_PUBLISHERS or reporter in EXCLUDE_REPORTERS:
                continue

            seen_links.add(link)
            collected.append(
                {
                    "title": clean_html(item.get("title", "")),
                    "description": clean_html(item.get("description", "")),
                    "link": link,
                    "pub_date": parse_pub_date(item.get("pubDate", "")),
                    "publisher": publisher,
                    "reporter": reporter,
                    "content": meta.get("content", ""),
                }
            )

            if len(collected) >= TARGET_COUNT:
                return collected

    return collected


def extract_tag_field(response_text: str, field_name: str, default_value: str) -> str:
    match = re.search(rf"{field_name}\s*:\s*([^\n\]]+)", response_text, flags=re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return default_value


def build_tag(publisher: str, reporter: str, region: str, keyword: str, signal: str) -> str:
    normalized_signal = signal.upper().strip()
    if normalized_signal not in {"BULL", "BEAR", "FLAT"}:
        normalized_signal = "FLAT"
    return f"[{publisher} | {reporter} | {region} | {keyword} | {normalized_signal}]"


def summarize_with_gemini(api_key: str, article: Dict[str, str]) -> Dict[str, str]:
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-1.5-flash")

    content = article.get("content") or article.get("description")
    
    # ğŸ’¡ ì—ëŸ¬ ë°©ì§€ 1: ê¸°ì‚¬ ë³¸ë¬¸ì„ 4000ìì—ì„œ 3000ìë¡œ ì‚´ì§ ì¤„ì—¬ì„œ í† í° ì´ˆê³¼(ìš©ëŸ‰ ì´ˆê³¼) ë°©ì§€
    prompt = f"""
ë„ˆëŠ” ë¶€ë™ì‚° ì‹œì¥ ì• ë„ë¦¬ìŠ¤íŠ¸ë‹¤.
ì•„ë˜ ê¸°ì‚¬ ë‚´ìš©ì„ 2~4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ìš”ì•½í•˜ê³ , ë§ˆì§€ë§‰ì— ë‹¤ìŒ ì •ë³´ë¥¼ ê°ê° í•œ ì¤„ë¡œ ì¶œë ¥í•˜ë¼:
Region: (í•œêµ­ ë‚´ ì£¼ìš” ì§€ì—­ ë˜ëŠ” ì „êµ­)
Keyword: (í•µì‹¬ë‹¨ì–´ 1~3ê°œ)
Signal: (BULL, BEAR, FLAT ì¤‘ í•˜ë‚˜)

ê¸°ì‚¬ ì œëª©: {article['title']}
ê¸°ì‚¬ ë³¸ë¬¸: {content[:3000]}
""".strip()

    try:
        response = model.generate_content(prompt)
        text = (response.text or "").strip()
    except Exception as exc:
        text = f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {exc}\nRegion: ì „êµ­\nKeyword: ë¶€ë™ì‚°\nSignal: FLAT"

    region = extract_tag_field(text, "Region", "ì „êµ­")
    keyword = extract_tag_field(text, "Keyword", "ë¶€ë™ì‚°")
    signal = extract_tag_field(text, "Signal", "FLAT")

    # ğŸ’¡ ì—ëŸ¬ ë°©ì§€ 2: ì œë¯¸ë‚˜ì´ê°€ ë‹µë³€ í˜•ì‹ì„ í‹€ë ¸ì„ ë•Œ ë°œìƒí•˜ëŠ” ì—ëŸ¬ ì²˜ë¦¬
    try:
        summary_part = re.split(r"\n\s*Region\s*:", text, maxsplit=1, flags=re.IGNORECASE)[0].strip()
    except Exception:
        summary_part = text

    tag = build_tag(article["publisher"], article["reporter"], region, keyword, signal)

    return {
        **article,
        "summary": f"{summary_part}\n\n{tag}",
        "region": region,
        "keyword": keyword,
        "signal": signal.upper() if signal.upper() in {"BULL", "BEAR", "FLAT"} else "FLAT",
        "tag": tag,
        "collected_at": datetime.utcnow().isoformat(),
    }


def save_news_data(rows: List[Dict[str, str]]) -> None:
    # ğŸ’¡ ì—ëŸ¬ ë°©ì§€ 3: ì €ì¥í•  ë°ì´í„°ê°€ 0ê±´ì¼ ë•Œ ì—ëŸ¬ ë‚˜ëŠ” ê²ƒ ë°©ì§€
    if not rows:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    new_df = pd.DataFrame(rows)

    if os.path.exists(CSV_PATH):
        existing_df = pd.read_csv(CSV_PATH)
        existing_links = set(existing_df.get("link", pd.Series(dtype=str)).dropna().tolist())
        append_df = new_df[~new_df["link"].isin(existing_links)]
        
        if append_df.empty:
            print("ìƒˆë¡œ ì¶”ê°€í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë‘ ì¤‘ë³µ).")
            return
            
        combined_df = pd.concat([existing_df, append_df], ignore_index=True)
        combined_df = combined_df.drop_duplicates(subset=["link"], keep="first")
        combined_df.to_csv(CSV_PATH, index=False, encoding="utf-8-sig")
        print(f"ê¸°ì¡´ {len(existing_df)}ê±´ + ì‹ ê·œ {len(append_df)}ê±´ ì €ì¥ ì™„ë£Œ")
    else:
        new_df.to_csv(CSV_PATH, index=False, encoding="utf-8-sig")
        print(f"ì‹ ê·œ {len(new_df)}ê±´ ì €ì¥ ì™„ë£Œ")


def main() -> None:
    client_id = get_env("NAVER_CLIENT_ID")
    client_secret = get_env("NAVER_CLIENT_SECRET")
    gemini_api_key = get_env("GEMINI_API_KEY")

    print(f"[{datetime.now()}] ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    articles = fetch_naver_news(client_id, client_secret)
    
    current_count = len(articles)
    if current_count == 0:
        print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ğŸ’¡ ì—ëŸ¬ ë°©ì§€ 4: 30ê±´ì´ ì•ˆ ë¼ë„ ì—ëŸ¬ ë„ìš°ì§€ ì•Šê³  ëª¨ì¸ ë§Œí¼ë§Œ ì²˜ë¦¬
    process_count = min(current_count, TARGET_COUNT)
    print(f"[{datetime.now()}] ì´ {current_count}ê±´ ì¤‘ {process_count}ê±´ ì œë¯¸ë‚˜ì´ ìš”ì•½ ì‹œì‘ (5ì´ˆ ê°„ê²©)")

    analyzed: List[Dict[str, str]] = []
    for i, article in enumerate(articles[:process_count]):
        print(f"[{i+1}/{process_count}] ìš”ì•½ ì¤‘: {article['title'][:30]}...")
        
        analyzed.append(summarize_with_gemini(gemini_api_key, article))
        
        # ğŸ’¡ í•µì‹¬ ì¿¨íƒ€ì„: ë§ˆì§€ë§‰ ê¸°ì‚¬ê°€ ì•„ë‹ ë•Œë§Œ 5ì´ˆ ëŒ€ê¸° (ë¬´ë£Œ ë²„ì „ ì œí•œ ë°©ì§€)
        if i < process_count - 1:
            time.sleep(5)

    save_news_data(analyzed)
    print(f"[{datetime.now()}] ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
