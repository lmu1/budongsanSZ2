import os
import re
import time
from datetime import datetime
from pathlib import Path

import pandas as pd
import requests
from bs4 import BeautifulSoup
from google import genai

# --- PRO ì„¤ì •ë¶€ ---
QUERY = "ë¶€ë™ì‚° ì „ë§"
TARGET_COUNT = 20  # í•˜ë£¨ í•œë„ ë°©ì–´ë¥¼ ìœ„í•œ 20ê°œ ì„¸íŒ…
TARGET_MODEL = "gemini-2.5-flash-lite"  # ì‚¬ìš©ìë‹˜ì´ ì„ íƒí•˜ì‹  2.5 ë²„ì „ ìœ ì§€
OUTPUT_FILES = ["news_data.csv", "news_data_latest.csv"]
CANONICAL_FILE = "news_data_latest.csv"
REQUIRED_COLUMNS = [
    "title",
    "link",
    "summary",
    "publisher",
    "reporter",
    "signal",
    "collected_at",
]

def get_env(name: str) -> str:
    return os.getenv(name, "")

def extract_article_metadata(link: str) -> dict:
    metadata = {"publisher": "Unknown", "reporter": "Unknown", "content": ""}
    try:
        resp = requests.get(link, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # 1. ë³¸ë¬¸ ì¶”ì¶œ
        content_node = (
            soup.select_one("article#dic_area")
            or soup.select_one("#newsct_article")
            or soup.select_one("#articleBodyContents")
        )
        if content_node:
            metadata["content"] = content_node.get_text(" ", strip=True)[:2500]
            
        # 2. ì–¸ë¡ ì‚¬ ì¶”ì¶œ
        pub_meta = soup.select_one("meta[property='og:site_name']")
        if pub_meta:
            metadata["publisher"] = pub_meta.get("content", "Unknown").strip()
            
        # ğŸ’¡ 3. ê¸°ì ì´ë¦„ ì¶”ì¶œ (ìƒˆë¡œ ì¶”ê°€ëœ ë¡œì§)
        reporter_node = (
            soup.select_one(".media_end_head_journalist_name") 
            or soup.select_one(".byline_s") 
            or soup.select_one(".b_text")
        )
        if reporter_node:
            metadata["reporter"] = reporter_node.get_text(strip=True)
            
    except Exception:
        pass
    return metadata

def find_all_news_csv() -> list[Path]:
    files = sorted(Path(".").glob("news_data*.csv"))
    return [f for f in files if f.name != CANONICAL_FILE]

def load_all_existing_news() -> pd.DataFrame:
    source_files = find_all_news_csv()
    frames: list[pd.DataFrame] = []

    for file in source_files:
        try:
            # ğŸ’¡ í•œê¸€ ê¹¨ì§ ë° ë¡œë”© ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ encoding ì¶”ê°€
            df = pd.read_csv(file, encoding="utf-8-sig")
            missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]
            if missing:
                print(f"âš ï¸ ìŠ¤í‚µ: {file} (í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing})")
                continue
            frames.append(df[REQUIRED_COLUMNS].copy())
        except Exception as err:
            print(f"âš ï¸ ìŠ¤í‚µ: {file} ë¡œë”© ì‹¤íŒ¨ ({err})")

    if frames:
        merged = pd.concat(frames, ignore_index=True)
        print(f"ğŸ“š ê¸°ì¡´ CSV ë³‘í•© ì™„ë£Œ: {len(source_files)}ê°œ íŒŒì¼ / {len(merged)}ê±´")
        return merged

    return pd.DataFrame(columns=REQUIRED_COLUMNS)

def build_canonical_dataset(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df

    summary = df["summary"].astype(str).str.strip()
    valid = summary.ne("") & summary.ne("nan") & summary.ne("None")
    df = df[valid].copy()

    df["__collected_at_dt"] = pd.to_datetime(df["collected_at"], errors="coerce")
    df = df.sort_values("__collected_at_dt")

    link = df["link"].astype(str).str.strip()
    has_link = link.ne("") & link.str.lower().ne("nan")

    with_link = df[has_link].drop_duplicates(subset=["link"], keep="last")
    without_link = df[~has_link].drop_duplicates(subset=["title", "summary"], keep="last")

    canonical = pd.concat([with_link, without_link], ignore_index=True)
    canonical = canonical.sort_values("__collected_at_dt", ascending=False)
    canonical = canonical.drop(columns=["__collected_at_dt"])
    return canonical[REQUIRED_COLUMNS]

def save_canonical(df: pd.DataFrame) -> None:
    for path in OUTPUT_FILES:
        df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"ğŸ‰ ëˆ„ì  ì €ì¥ ì„±ê³µ! ì´ {len(df)}ê±´ì˜ DBê°€ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main() -> None:
    client_id = get_env("NAVER_CLIENT_ID")
    client_secret = get_env("NAVER_CLIENT_SECRET")
    api_key = get_env("GEMINI_API_KEY")

    if not api_key:
        print("âŒ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    client = genai.Client(api_key=api_key)
    print(f"ğŸš€ í”„ë¡œë²„ì „ ìˆ˜ì§‘ê¸° ê°€ë™: {TARGET_MODEL}")

    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    res = requests.get(
        "https://openapi.naver.com/v1/search/news.json",
        headers=headers,
        params={"query": QUERY, "display": 100, "sort": "date"},
    )
    items = res.json().get("items", [])

    new_analyzed = []
    error_count = 0

    for item in items:
        if len(new_analyzed) >= TARGET_COUNT or error_count > 5:
            break

        link = item.get("originallink") or item.get("link")
        meta = extract_article_metadata(link)

        prompt = f"""ë¶€ë™ì‚° ì „ë¬¸ê°€ë¡œì„œ ì•„ë˜ ê¸°ì‚¬ë¥¼ 3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´. ì •ì¹˜/ì‚¬ê±´ ê¸°ì‚¬ë©´ "Signal: INVALID"ë¼ê³  ë‹µí•´.
ì œëª©: {item['title']}
ë³¸ë¬¸: {meta['content']}

[í•„ìˆ˜í˜•ì‹]
Region: ì§€ì—­ëª…
Keyword: í•µì‹¬í‚¤ì›Œë“œ
Signal: (BULL/BEAR/FLAT)"""

        try:
            print(f"â³ AI ë¶„ì„ ì¤‘... ({len(new_analyzed)+1}/{TARGET_COUNT})")
            time.sleep(30)  # 2.5 ë²„ì „ í•œë„(RPM) ë°©ì–´
            response = client.models.generate_content(model=TARGET_MODEL, contents=prompt)
            text = response.text

            if "INVALID" in text.upper():
                continue

            signal = "FLAT"
            if "BULL" in text.upper():
                signal = "BULL"
            elif "BEAR" in text.upper():
                signal = "BEAR"

            new_analyzed.append(
                {
                    "title": re.sub(r"<[^>]+>", "", item["title"]),
                    "link": link,
                    "summary": text.strip(),
                    "publisher": meta["publisher"],
                    "reporter": meta["reporter"],
                    "signal": signal,
                    "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
                }
            )
            print(f"âœ… ì™„ë£Œ: {item['title'][:15]}...")

        except Exception as err:
            print(f"âš ï¸ ì˜¤ë¥˜: {err}")
            if "429" in str(err):
                break
            error_count += 1

    existing_df = load_all_existing_news()

    if new_analyzed:
        new_df = pd.DataFrame(new_analyzed)
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
    else:
        combined_df = existing_df
        print("â„¹ï¸ ì‹ ê·œ ë¶„ì„ ê¸°ì‚¬ê°€ ì—†ì–´ ê¸°ì¡´ ëˆ„ì ë³¸ë§Œ ì •ë¦¬í•©ë‹ˆë‹¤.")

    canonical_df = build_canonical_dataset(combined_df)
    save_canonical(canonical_df)

if __name__ == "__main__":
    main()
