diff --git a/main.py b/main.py
index 7407a25be810fba9db4c4edb75d56636eab70417..4189bcdd7ac018cec941bd3bb414d310a87946b5 100644
--- a/main.py
+++ b/main.py
@@ -1,119 +1,197 @@
 import os
 import re
 import time
 from datetime import datetime
+from pathlib import Path
+
 import pandas as pd
 import requests
 from bs4 import BeautifulSoup
 from google import genai
 
 # --- PRO ì„¤ì •ë¶€ ---
 QUERY = "ë¶€ë™ì‚° ì „ë§"
 TARGET_COUNT = 20  # í•˜ë£¨ í•œë„ ë°©ì–´ë¥¼ ìœ„í•œ 20ê°œ ì„¸íŒ…
-CSV_PATH = "news_data.csv"
 TARGET_MODEL = "gemini-2.5-flash"  # ì‚¬ìš©ìë‹˜ì´ ì„ íƒí•˜ì‹  2.5 ë²„ì „ ìœ ì§€
+OUTPUT_FILES = ["news_data.csv", "news_data_latest.csv"]
+CANONICAL_FILE = "news_data_latest.csv"
+REQUIRED_COLUMNS = [
+    "title",
+    "link",
+    "summary",
+    "publisher",
+    "reporter",
+    "signal",
+    "collected_at",
+]
+
 
 def get_env(name: str) -> str:
     return os.getenv(name, "")
 
+
 def extract_article_metadata(link: str) -> dict:
     metadata = {"publisher": "Unknown", "reporter": "Unknown", "content": ""}
     try:
         resp = requests.get(link, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
         soup = BeautifulSoup(resp.text, "html.parser")
-        content_node = soup.select_one("article#dic_area") or soup.select_one("#newsct_article") or soup.select_one("#articleBodyContents")
+        content_node = (
+            soup.select_one("article#dic_area")
+            or soup.select_one("#newsct_article")
+            or soup.select_one("#articleBodyContents")
+        )
         if content_node:
             metadata["content"] = content_node.get_text(" ", strip=True)[:2500]
         pub_meta = soup.select_one("meta[property='og:site_name']")
         if pub_meta:
             metadata["publisher"] = pub_meta.get("content", "Unknown").strip()
-    except:
+    except Exception:
         pass
     return metadata
 
-def main():
+
+def find_all_news_csv() -> list[Path]:
+    files = sorted(Path(".").glob("news_data*.csv"))
+    return [f for f in files if f.name != CANONICAL_FILE]
+
+
+def load_all_existing_news() -> pd.DataFrame:
+    source_files = find_all_news_csv()
+    frames: list[pd.DataFrame] = []
+
+    for file in source_files:
+        try:
+            df = pd.read_csv(file)
+            missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]
+            if missing:
+                print(f"âš ï¸ ìŠ¤í‚µ: {file} (í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing})")
+                continue
+            frames.append(df[REQUIRED_COLUMNS].copy())
+        except Exception as err:
+            print(f"âš ï¸ ìŠ¤í‚µ: {file} ë¡œë”© ì‹¤íŒ¨ ({err})")
+
+    if frames:
+        merged = pd.concat(frames, ignore_index=True)
+        print(f"ğŸ“š ê¸°ì¡´ CSV ë³‘í•© ì™„ë£Œ: {len(source_files)}ê°œ íŒŒì¼ / {len(merged)}ê±´")
+        return merged
+
+    return pd.DataFrame(columns=REQUIRED_COLUMNS)
+
+
+def build_canonical_dataset(df: pd.DataFrame) -> pd.DataFrame:
+    if df.empty:
+        return df
+
+    summary = df["summary"].astype(str).str.strip()
+    valid = summary.ne("") & summary.ne("nan") & summary.ne("None")
+    df = df[valid].copy()
+
+    df["__collected_at_dt"] = pd.to_datetime(df["collected_at"], errors="coerce")
+    df = df.sort_values("__collected_at_dt")
+
+    link = df["link"].astype(str).str.strip()
+    has_link = link.ne("") & link.str.lower().ne("nan")
+
+    with_link = df[has_link].drop_duplicates(subset=["link"], keep="last")
+    without_link = df[~has_link].drop_duplicates(subset=["title", "summary"], keep="last")
+
+    canonical = pd.concat([with_link, without_link], ignore_index=True)
+    canonical = canonical.sort_values("__collected_at_dt", ascending=False)
+    canonical = canonical.drop(columns=["__collected_at_dt"])
+    return canonical[REQUIRED_COLUMNS]
+
+
+def save_canonical(df: pd.DataFrame) -> None:
+    for path in OUTPUT_FILES:
+        df.to_csv(path, index=False, encoding="utf-8-sig")
+    print(f"ğŸ‰ ëˆ„ì  ì €ì¥ ì„±ê³µ! ì´ {len(df)}ê±´ì˜ DBê°€ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.")
+
+
+def main() -> None:
     client_id = get_env("NAVER_CLIENT_ID")
     client_secret = get_env("NAVER_CLIENT_SECRET")
     api_key = get_env("GEMINI_API_KEY")
 
     if not api_key:
         print("âŒ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
         return
 
     client = genai.Client(api_key=api_key)
     print(f"ğŸš€ í”„ë¡œë²„ì „ ìˆ˜ì§‘ê¸° ê°€ë™: {TARGET_MODEL}")
 
     headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
-    res = requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params={"query": QUERY, "display": 100, "sort": "date"})
+    res = requests.get(
+        "https://openapi.naver.com/v1/search/news.json",
+        headers=headers,
+        params={"query": QUERY, "display": 100, "sort": "date"},
+    )
     items = res.json().get("items", [])
 
     new_analyzed = []
-    error_count = 0 
+    error_count = 0
 
     for item in items:
         if len(new_analyzed) >= TARGET_COUNT or error_count > 5:
             break
-        
+
         link = item.get("originallink") or item.get("link")
         meta = extract_article_metadata(link)
-        
-        prompt = f"""ë¶€ë™ì‚° ì „ë¬¸ê°€ë¡œì„œ ì•„ë˜ ê¸°ì‚¬ë¥¼ 3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´. ì •ì¹˜/ì‚¬ê±´ ê¸°ì‚¬ë©´ "Signal: INVALID"ë¼ê³  ë‹µí•´.
+
+        prompt = f"""ë¶€ë™ì‚° ì „ë¬¸ê°€ë¡œì„œ ì•„ë˜ ê¸°ì‚¬ë¥¼ 3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´. ì •ì¹˜/ì‚¬ê±´ ê¸°ì‚¬ë©´ \"Signal: INVALID\"ë¼ê³  ë‹µí•´.
 ì œëª©: {item['title']}
 ë³¸ë¬¸: {meta['content']}
 
 [í•„ìˆ˜í˜•ì‹]
 Region: ì§€ì—­ëª…
 Keyword: í•µì‹¬í‚¤ì›Œë“œ
 Signal: (BULL/BEAR/FLAT)"""
 
         try:
             print(f"â³ AI ë¶„ì„ ì¤‘... ({len(new_analyzed)+1}/{TARGET_COUNT})")
-            time.sleep(10) # 2.5 ë²„ì „ í•œë„(RPM) ë°©ì–´
+            time.sleep(10)  # 2.5 ë²„ì „ í•œë„(RPM) ë°©ì–´
             response = client.models.generate_content(model=TARGET_MODEL, contents=prompt)
             text = response.text
-            
-            if "INVALID" in text.upper(): continue
+
+            if "INVALID" in text.upper():
+                continue
 
             signal = "FLAT"
-            if "BULL" in text.upper(): signal = "BULL"
-            elif "BEAR" in text.upper(): signal = "BEAR"
-
-            new_analyzed.append({
-                "title": re.sub(r"<[^>]+>", "", item['title']),
-                "link": link,
-                "summary": text.strip(),
-                "publisher": meta['publisher'],
-                "reporter": meta['reporter'],
-                "signal": signal,
-                "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M")
-            })
+            if "BULL" in text.upper():
+                signal = "BULL"
+            elif "BEAR" in text.upper():
+                signal = "BEAR"
+
+            new_analyzed.append(
+                {
+                    "title": re.sub(r"<[^>]+>", "", item["title"]),
+                    "link": link,
+                    "summary": text.strip(),
+                    "publisher": meta["publisher"],
+                    "reporter": meta["reporter"],
+                    "signal": signal,
+                    "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
+                }
+            )
             print(f"âœ… ì™„ë£Œ: {item['title'][:15]}...")
 
-        except Exception as e:
-            print(f"âš ï¸ ì˜¤ë¥˜: {e}")
-            if "429" in str(e): break
+        except Exception as err:
+            print(f"âš ï¸ ì˜¤ë¥˜: {err}")
+            if "429" in str(err):
+                break
             error_count += 1
 
-    # ğŸ”¥ [PRO] ì™„ë²½í•œ ë°ì´í„° ëˆ„ì  ë¡œì§
+    existing_df = load_all_existing_news()
+
     if new_analyzed:
         new_df = pd.DataFrame(new_analyzed)
-        if os.path.exists(CSV_PATH):
-            try:
-                old_df = pd.read_csv(CSV_PATH)
-                combined_df = pd.concat([old_df, new_df], ignore_index=True)
-                # ë§í¬(link)ê°€ ê°™ì€ ì¤‘ë³µ ê¸°ì‚¬ëŠ” ìµœì‹  ìˆ˜ì§‘ë³¸ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê³  ì‚­ì œ
-                combined_df = combined_df.drop_duplicates(subset=['link'], keep='last')
-            except:
-                combined_df = new_df
-        else:
-            combined_df = new_df
-
-        # ìµœì‹  ê¸°ì‚¬ê°€ ìœ„ë¡œ ì˜¤ë„ë¡ ì •ë ¬ í›„ ì €ì¥
-        combined_df = combined_df.sort_values(by="collected_at", ascending=False)
-        combined_df.to_csv(CSV_PATH, index=False, encoding="utf-8-sig")
-        print(f"ğŸ‰ ëˆ„ì  ì €ì¥ ì„±ê³µ! ì´ {len(combined_df)}ê±´ì˜ DBê°€ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.")
+        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
     else:
-        print("ì €ì¥í•  ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
+        combined_df = existing_df
+        print("â„¹ï¸ ì‹ ê·œ ë¶„ì„ ê¸°ì‚¬ê°€ ì—†ì–´ ê¸°ì¡´ ëˆ„ì ë³¸ë§Œ ì •ë¦¬í•©ë‹ˆë‹¤.")
+
+    canonical_df = build_canonical_dataset(combined_df)
+    save_canonical(canonical_df)
+
 
 if __name__ == "__main__":
     main()
