import html
import os
import re
import time
from datetime import datetime
from email.utils import parsedate_to_datetime
from typing import Dict, List, Optional

import google.generativeai as genai
import pandas as pd
import requests
from bs4 import BeautifulSoup

EXCLUDE_PUBLISHERS: List[str] = []
EXCLUDE_REPORTERS: List[str] = []

NAVER_API_URL = "https://openapi.naver.com/v1/search/news.json"
QUERY = "ë¶€ë™ì‚° ì „ë§"
TARGET_COUNT = 30
CSV_PATH = "news_data.csv"

def get_env(name: str) -> str:
    value = os.getenv(name)
    if not value:
        raise EnvironmentError(f"í™˜ê²½ë³€ìˆ˜ {name} ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return value

def clean_html(raw_text: str) -> str:
    no_tag = re.sub(r"<[^>]+>", "", raw_text or "")
    return html.unescape(no_tag).strip()

def parse_pub_date(pub_date: str) -> str:
    try:
        dt = parsedate_to_datetime(pub_date)
        return dt.isoformat()
    except Exception:
        return datetime.utcnow().isoformat()

def extract_article_metadata(link: str) -> Dict[str, str]:
    metadata = {"publisher": "Unknown", "reporter": "Unknown", "content": ""}
    try:
        resp = requests.get(link, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        publisher_candidates = [
            soup.select_one("meta[property='og:article:author']"),
            soup.select_one("meta[name='twitter:creator']"),
            soup.select_one("meta[property='og:site_name']"),
            soup.select_one("meta[name='newsct']"),
            soup.select_one("a.media_end_head_top_logo img"),
            soup.select_one("img.media_end_head_top_logo_img"),
        ]
        for candidate in publisher_candidates:
            if not candidate: continue
            value = candidate.get("content") or candidate.get("alt") or candidate.get_text(strip=True)
            if value:
                metadata["publisher"] = value.strip()
                break

        reporter_candidates = [
            soup.select_one("meta[name='byl']"),
            soup.select_one(".media_end_head_journalist_name"),
            soup.select_one(".byline_s"),
        ]
        for candidate in reporter_candidates:
            if not candidate: continue
            value = candidate.get("content") or candidate.get_text(" ", strip=True)
            if value:
                metadata["reporter"] = re.sub(r"ê¸°ì.*$", "ê¸°ì", value).strip()
                break

        content_node = soup.select_one("article#dic_area") or soup.select_one("#newsct_article") or soup.select_one("#articleBodyContents")
        if content_node:
            metadata["content"] = re.sub(r"\s+", " ", content_node.get_text(" ", strip=True))
    except Exception:
        pass
    return metadata

def fetch_naver_news(client_id: str, client_secret: str) -> List[Dict[str, str]]:
    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    collected = []
    seen_links = set()

    for start in range(1, 1000, 100):
        params = {"query": QUERY, "display": 100, "start": start, "sort": "date"}
        res = requests.get(NAVER_API_URL, headers=headers, params=params, timeout=10)
        res.raise_for_status()
        items = res.json().get("items", [])
        if not items: break

        for item in items:
            link = item.get("originallink") or item.get("link")
            if not link or link in seen_links: continue
            
            meta = extract_article_metadata(link)
            if meta["publisher"] in EXCLUDE_PUBLISHERS or meta["reporter"] in EXCLUDE_REPORTERS: continue
            
            seen_links.add(link)
            collected.append({
                "title": clean_html(item.get("title", "")),
                "description": clean_html(item.get("description", "")),
                "link": link,
                "pub_date": parse_pub_date(item.get("pubDate", "")),
                "publisher": meta["publisher"],
                "reporter": meta["reporter"],
                "content": meta["content"],
            })
            
            # ì¤‘ê°„ì— AIê°€ ê±°ë¥¼ ê²ƒì„ ëŒ€ë¹„í•´ ëª©í‘œì¹˜ë³´ë‹¤ ë„‰ë„‰í•˜ê²Œ ê¸°ì‚¬ë¥¼ ëª¨ì•„ë‘¡ë‹ˆë‹¤.
            if len(collected) >= TARGET_COUNT * 2: 
                return collected
    return collected

def extract_tag_field(response_text: str, field_name: str, default_value: str) -> str:
    match = re.search(rf"{field_name}\s*:\s*([^\n\]]+)", response_text, flags=re.IGNORECASE)
    return match.group(1).strip() if match else default_value

def build_tag(publisher: str, reporter: str, region: str, keyword: str, signal: str) -> str:
    sig = signal.upper().strip() if signal.upper().strip() in {"BULL", "BEAR", "FLAT"} else "FLAT"
    return f"[{publisher} | {reporter} | {region} | {keyword} | {sig}]"

def summarize_with_gemini(api_key: str, article: Dict[str, str]) -> Optional[Dict[str, str]]:
    genai.configure(api_key=api_key)
    # ğŸ’¡ 1.5 Pro ëª¨ë¸ ì ìš©
    model = genai.GenerativeModel("gemini-1.5-flash")

    content = article.get("content") or article.get("description")
    
    # ğŸ”¥ AI íŒë‹¨ í•„í„°: ë¬¸ë§¥ì„ ì½ê³  ë¶€ë™ì‚°ê³¼ ë¬´ê´€í•˜ë©´ INVALID ë°˜í™˜
    prompt = f"""
ë„ˆëŠ” ìµœê³ ì˜ ë¶€ë™ì‚° ì‹œì¥ ì• ë„ë¦¬ìŠ¤íŠ¸ë‹¤.
ì•„ë˜ ê¸°ì‚¬ê°€ 'ë¶€ë™ì‚° ì‹œì¥ ë™í–¥, ê°€ê²©, ì •ì±…, ì „ë§'ê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ìˆëŠ”ì§€ ë¨¼ì € íŒë‹¨í•˜ë¼.
ë§Œì•½ ë¶€ë™ì‚°ê³¼ ë¬´ê´€í•œ ì •ì¹˜, ë²”ì£„, ë‹¨ìˆœ ì‚¬íšŒ ê¸°ì‚¬ë¼ë©´ ìš”ì•½í•˜ì§€ ë§ê³  ë‹¨ í•œ ì¤„ë¡œ ì•„ë˜ì™€ ê°™ì´ ì¶œë ¥í•˜ë¼:
Signal: INVALID

ì§„ì§œ ë¶€ë™ì‚° ê¸°ì‚¬ê°€ ë§ë‹¤ë©´, ë‚´ìš©ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ 2~4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ê³  ë§ˆì§€ë§‰ì— ë‹¤ìŒ ì •ë³´ë¥¼ í•œ ì¤„ì”© ì¶œë ¥í•˜ë¼:
Region: (í•œêµ­ ë‚´ ì£¼ìš” ì§€ì—­ ë˜ëŠ” ì „êµ­)
Keyword: (í•µì‹¬ë‹¨ì–´ 1~3ê°œ)
Signal: (BULL, BEAR, FLAT ì¤‘ í•˜ë‚˜)

ê¸°ì‚¬ ì œëª©: {article['title']}
ê¸°ì‚¬ ë³¸ë¬¸: {content[:3000]}
""".strip()

    try:
        response = model.generate_content(prompt)
        text = (response.text or "").strip()
    except Exception as exc:
        text = f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {exc}\nRegion: ì „êµ­\nKeyword: ë¶€ë™ì‚°\nSignal: FLAT"

    signal = extract_tag_field(text, "Signal", "FLAT").upper()
    
    # ë¬´ê´€í•œ ê¸°ì‚¬ë¡œ íŒë‹¨ë˜ë©´ Noneì„ ë°˜í™˜í•´ì„œ ì»·!
    if "INVALID" in signal:
        return None

    region = extract_tag_field(text, "Region", "ì „êµ­")
    keyword = extract_tag_field(text, "Keyword", "ë¶€ë™ì‚°")

    try:
        summary_part = re.split(r"\n\s*Region\s*:", text, maxsplit=1, flags=re.IGNORECASE)[0].strip()
    except Exception:
        summary_part = text

    tag = build_tag(article["publisher"], article["reporter"], region, keyword, signal)

    return {
        **article,
        "summary": f"{summary_part}\n\n{tag}",
        "region": region,
        "keyword": keyword,
        "signal": signal if signal in {"BULL", "BEAR", "FLAT"} else "FLAT",
        "tag": tag,
        "collected_at": datetime.utcnow().isoformat(),
    }

def save_news_data(rows: List[Dict[str, str]]) -> None:
    if not rows:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    new_df = pd.DataFrame(rows)
    if os.path.exists(CSV_PATH):
        existing_df = pd.read_csv(CSV_PATH)
        existing_links = set(existing_df.get("link", pd.Series(dtype=str)).dropna().tolist())
        append_df = new_df[~new_df["link"].isin(existing_links)]
        if append_df.empty:
            print("ìƒˆë¡œ ì¶”ê°€í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë‘ ì¤‘ë³µ).")
            return
        combined_df = pd.concat([existing_df, append_df], ignore_index=True)
        combined_df = combined_df.drop_duplicates(subset=["link"], keep="first")
        combined_df.to_csv(CSV_PATH, index=False, encoding="utf-8-sig")
        print(f"ê¸°ì¡´ {len(existing_df)}ê±´ + ì‹ ê·œ {len(append_df)}ê±´ ì €ì¥ ì™„ë£Œ")
    else:
        new_df.to_csv(CSV_PATH, index=False, encoding="utf-8-sig")
        print(f"ì‹ ê·œ {len(new_df)}ê±´ ì €ì¥ ì™„ë£Œ")

def main() -> None:
    client_id = get_env("NAVER_CLIENT_ID")
    client_secret = get_env("NAVER_CLIENT_SECRET")
    gemini_api_key = get_env("GEMINI_API_KEY")

    print(f"[{datetime.now()}] ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    articles = fetch_naver_news(client_id, client_secret)
    
    if not articles:
        print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"[{datetime.now()}] ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì¤‘ {TARGET_COUNT}ê±´ ì—„ì„  ìš”ì•½ ì‹œì‘ (AI ë¬¸ë§¥ í•„í„° ì‘ë™ ì¤‘)")
    analyzed: List[Dict[str, str]] = []
    
    for article in articles:
        # ëª©í‘œì¹˜(30ê±´)ë¥¼ ì±„ì› ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
        if len(analyzed) >= TARGET_COUNT:
            break
            
        print(f"ê²€í†  ì¤‘: {article['title'][:30]}...")
        summary_data = summarize_with_gemini(gemini_api_key, article)
        
        # AIê°€ ë¬´ê´€í•˜ë‹¤ê³  íŒë‹¨(None ë°˜í™˜)í•˜ë©´ ì €ì¥í•˜ì§€ ì•Šê³  ë‹¤ìŒ ê¸°ì‚¬ë¡œ ë„˜ì–´ê°
        if summary_data is None:
            print(" â” ğŸš« [ì •ì¹˜/ë¬´ê´€ ê¸°ì‚¬] AIê°€ ê±¸ëŸ¬ëƒ„!")
            time.sleep(2)  
            continue
            
        analyzed.append(summary_data)
        print(f" â” âœ… [ì™„ë£Œ] (í˜„ì¬ {len(analyzed)}/{TARGET_COUNT}ê±´ í™•ì •)")
        time.sleep(5)

    save_news_data(analyzed)
    print(f"[{datetime.now()}] ì° ë¶€ë™ì‚° ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘ ë° ìš”ì•½ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
